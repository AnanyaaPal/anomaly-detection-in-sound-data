{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5b510ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, Model, optimizers\n",
    "import librosa\n",
    "\n",
    "\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a98e135b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "sample_rate = 16000\n",
    "print(type(sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38158207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/AnanyaPal1/Documents/TUD/5th semester/resampling and simulations/anomaly-detection-in-sound-data')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mention absolute import path w.r.t the current directory\n",
    "\n",
    "base_dir = Path.cwd().parent.parent\n",
    "base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e12c0704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Parameter (alle als Integer, wo nötig)\n",
    "\n",
    "sample_rate = 16000\n",
    "n_fft = 1024\n",
    "hop_length = 512\n",
    "n_mels = 128\n",
    "\n",
    "P = 2                       # Kontext links/rechts\n",
    "n_context = 2 * P + 1          # 5 Frames\n",
    "input_dim = n_mels * n_context    # 128 * 5 = 640\n",
    "\n",
    "train_dir = base_dir / \"data\" / \"fan\" / \"train\"\n",
    "test_dir  = base_dir / \"data\" / \"fan\" / \"test\"\n",
    "\n",
    "epochs_ae = 100\n",
    "batch_size_ae = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37a93106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/AnanyaPal1/Documents/TUD/5th semester/resampling and simulations/anomaly-detection-in-sound-data/data/fan/train')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0753d80",
   "metadata": {},
   "source": [
    "Compute context stacked log-mel features for the audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf08a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logmel_context(file_path,\n",
    "                           sample_rate=16000,\n",
    "                           n_fft=1024,\n",
    "                           hop_length=512,\n",
    "                           n_mels=128,\n",
    "                           P=2):\n",
    "    \"\"\"\n",
    "    Compute context-stacked log-mel features for a single audio file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out : np.ndarray of shape (T_eff, n_mels * (2P+1))\n",
    "        Each row is a flattened (n_mels x (2P+1)) log-mel context window.\n",
    "        Returns None if the file is too short to form one full context window.\n",
    "    \"\"\"\n",
    "    n_context = 2 * P + 1\n",
    "    input_dim = n_mels * n_context\n",
    "\n",
    "    # 1) Load waveform (mono) at target sample rate\n",
    "    y, sr = librosa.load(file_path, sr=sample_rate, mono=True)\n",
    "\n",
    "    # 2) Mel spectrogram (power)\n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=y,\n",
    "        sr=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels\n",
    "    )\n",
    "\n",
    "    # 3) Convert power -> dB (log scale), reference to max (like ref=np.max)\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "    # Ensure shape is (n_mels, T)\n",
    "    M = np.asarray(mel_db)\n",
    "    if M.ndim != 2:\n",
    "        M = M.reshape(n_mels, -1)\n",
    "\n",
    "    n_frames = M.shape[1]\n",
    "\n",
    "    # 4) Discard too-short files\n",
    "    if n_frames < n_context:\n",
    "        return None\n",
    "\n",
    "    # 5) Build context-stacked features\n",
    "    T_eff = n_frames - 2 * P\n",
    "    out = np.zeros((T_eff, input_dim), dtype=np.float32)\n",
    "\n",
    "    idx = 0\n",
    "    for t in range(P, n_frames - P):  # 0-based: centers P .. n_frames-P-1\n",
    "        win = M[:, (t - P):(t + P + 1)]      # shape: (n_mels, n_context)\n",
    "        out[idx, :] = win.reshape(-1, order=\"F\")  # match R as.numeric() column-major flatten\n",
    "        idx += 1\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c38831",
   "metadata": {},
   "source": [
    "Load Data and ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ef9cd6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'number of train files:3675'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'number of test files: 1875'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_files = list(train_dir.rglob(\"*.wav\"))\n",
    "test_files  = list(test_dir.rglob(\"*.wav\"))\n",
    "\n",
    "display(f\"number of train files:{len(train_files)}\")\n",
    "display(f\"number of test files: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f76e0e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ea2347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(x):\n",
    "    return re.sub(r\".*id_(..)_.*\", r\"\\1\", os.path.basename(x))\n",
    "\n",
    "train_ids  =  [get_id(p) for p in train_files]\n",
    "test_ids   = [get_id(p) for p in test_files]\n",
    "unique_ids = sorted(set(train_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fa4a06",
   "metadata": {},
   "source": [
    "Implement an AE.\n",
    "\n",
    "\n",
    "Remark: Settings specific to the structure of the AE (hyperparameters, structure) are taken from the benchmark AE method from the following research paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b412e2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim):\n",
    "\n",
    "    input = layers.Input(shape=(input_dim,))\n",
    "\n",
    "    # Encoder\n",
    "\n",
    "    x = layers.Dense(128)(input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.Dense(128)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.Dense(128)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.Dense(128)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    # Bottleneck 8D\n",
    "    x = layers.Dense(8)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = layers.Dense(128)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.Dense(128)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.Dense(128)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.Dense(128)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    output = layers.Dense(input_dim)(x)\n",
    "\n",
    "    model = keras.Model(input, output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss = \"mse\"\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e40326b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> (3675,)\n",
      "<class 'list'> (1875,)\n",
      "['04', '00', '06', '00', '06', '02', '04', '04', '06', '02', '04', '04', '02', '00', '00', '04', '02', '02', '04', '06', '06', '00', '04', '00', '06', '00', '00', '06', '04', '06', '00', '00', '06', '02', '04', '06', '00', '06', '00', '02', '04', '04', '02', '04', '02', '02', '04', '06', '00', '00', '06', '00', '06', '04', '02', '06', '00', '00', '06', '06', '00', '06', '00', '04', '02', '04', '02', '02', '04', '02', '04', '04', '02', '00', '06', '06', '00', '06', '00', '00', '06', '06', '00', '04', '02', '06', '00', '06', '02', '02', '00', '06', '06', '04', '02', '02', '04', '00', '02', '04', '04', '02', '06', '00', '06', '04', '02', '06', '00', '06', '00', '02', '02', '04', '00', '06', '06', '00', '02', '02', '00', '06', '04', '02', '02', '00', '06', '06', '00', '04', '06', '00', '00', '06', '02', '04', '02', '06', '02', '04', '02', '04', '06', '00', '04', '02', '02', '06', '00', '06', '00', '06', '02', '04', '02', '00', '06', '00', '06', '06', '00', '06', '00', '00', '06', '04', '02', '02', '04', '02', '04', '04', '02', '00', '06', '02', '04', '00', '06', '06', '00', '02', '04', '06', '00', '00', '06', '00', '06', '06', '00', '02', '04', '04', '02', '04', '02', '02', '04', '00', '06', '00', '06', '04', '02', '06', '00', '06', '00', '00', '06', '04', '04', '00', '06', '00', '06', '00', '02', '04', '00', '06', '04', '02', '04', '02', '06', '00', '02', '04', '06', '00', '04', '04', '02', '04', '06', '00', '04', '00', '06', '00', '00', '06', '04', '02', '04', '02', '04', '06', '00', '04', '02', '04', '02', '00', '06', '02', '04', '04', '02', '04', '06', '00', '00', '06', '00', '06', '06', '00', '04', '04', '06', '00', '06', '00', '04', '02', '06', '00', '02', '04', '04', '02', '04', '02', '02', '04', '06', '00', '00', '06', '00', '06', '06', '00', '06', '00', '06', '00', '06', '00', '02', '04', '04', '02', '02', '04', '02', '04', '04', '02', '04', '02', '00', '06', '06', '00', '06', '00', '00', '06', '00', '06', '00', '06', '02', '04', '02', '02', '00', '06', '04', '02', '06', '02', '04', '02', '04', '00', '06', '04', '02', '00', '06', '06', '00', '02', '02', '06', '00', '00', '06', '06', '04', '00', '06', '04', '04', '06', '00', '00', '06', '02', '04', '06', '06', '00', '06', '00', '02', '00', '06', '00', '04', '02', '02', '04', '06', '06', '02', '04', '04', '02', '02', '06', '00', '06', '00', '02', '04', '02', '06', '00', '00', '06', '06', '00', '06', '00', '02', '04', '00', '06', '04', '02', '02', '04', '02', '04', '04', '02', '04', '02', '00', '06', '00', '06', '00', '06', '06', '00', '00', '06', '00', '06', '06', '00', '02', '04', '04', '02', '04', '02', '02', '04', '00', '06', '00', '06', '02', '04', '00', '06', '04', '06', '00', '04', '00', '06', '00', '06', '04', '02', '04', '06', '00', '00', '02', '04', '04', '02', '06', '06', '04', '02', '02', '04', '00', '02', '04', '06', '00', '00', '04', '04', '06', '00', '00', '02', '04', '04', '02', '06', '04', '02', '02', '04', '00', '02', '04', '00', '06', '00', '04', '04', '06', '00', '00', '06', '04', '06', '00', '04', '00', '06', '00', '06', '04', '04', '02', '06', '00', '02', '04', '04', '02', '04', '02', '02', '04', '00', '06', '00', '06', '02', '04', '00', '06', '06', '00', '00', '06', '00', '06', '06', '00', '04', '02', '02', '04', '02', '04', '04', '02', '04', '02', '00', '06', '00', '06', '06', '00', '00', '06', '06', '00', '06', '00', '02', '04', '00', '06', '00', '04', '02', '02', '04', '06', '06', '02', '04', '04', '02', '00', '02', '06', '00', '06', '00', '04', '02', '02', '04', '06', '00', '00', '06', '02', '04', '06', '00', '06', '06', '00', '04', '02', '00', '06', '04', '00', '06', '06', '00', '02', '02', '06', '00', '06', '00', '06', '04', '00', '06', '04', '00', '06', '02', '02', '04', '02', '00', '06', '04', '02', '06', '00', '02', '04', '02', '04', '06', '04', '02', '04', '02', '00', '06', '06', '00', '06', '00', '00', '06', '00', '06', '06', '00', '06', '00', '02', '04', '04', '02', '02', '04', '02', '04', '04', '02', '06', '00', '00', '06', '00', '06', '06', '00', '06', '00', '06', '00', '04', '02', '06', '00', '02', '04', '04', '02', '04', '02', '02', '04', '02', '04', '04', '06', '00', '00', '06', '00', '06', '06', '00', '04', '04', '06', '00', '04', '00', '06', '00', '06', '00', '04', '02', '04', '02', '04', '00', '04', '02', '04', '02', '00', '06', '02', '04', '02', '04', '00', '06', '04', '02', '04', '02', '06', '00', '02', '04', '06', '00', '04', '02', '04', '04', '06', '00', '06', '00', '02', '06', '00', '00', '06', '04', '04', '00', '06', '06', '00', '00', '02', '04', '04', '02', '04', '02', '02', '04', '00', '06', '00', '06', '04', '02', '06', '00', '02', '04', '06', '00', '00', '06', '00', '06', '06', '00', '04', '02', '02', '04', '02', '04', '04', '02', '00', '06', '02', '04', '00', '06', '00', '06', '00', '06', '06', '00', '06', '00', '00', '06', '04', '02', '00', '06', '02', '04', '02', '04', '06', '04', '02', '02', '06', '00', '06', '06', '00', '02', '04', '02', '02', '00', '06', '02', '04', '02', '00', '06', '06', '00', '04', '06', '00', '00', '06', '02', '00', '06', '04', '02', '06', '00', '06', '00', '02', '04', '02', '04', '00', '06', '06', '00', '02', '04', '02', '00', '06', '06', '02', '02', '00', '06', '06', '04', '02', '02', '04', '00', '02', '04', '04', '02', '06', '00', '06', '06', '00', '06', '00', '00', '06', '06', '00', '06', '00', '06', '00', '04', '02', '04', '02', '02', '04', '02', '04', '04', '02', '06', '00', '00', '06', '00', '06', '04', '02', '06', '00', '00', '06', '02', '04', '06', '00', '06', '00', '02', '04', '04', '02', '04', '02', '02', '04', '06', '00', '04', '00', '00', '06', '00', '06', '04', '06', '00', '02', '00', '06', '04', '00', '06', '00', '06', '04', '02', '04', '06', '02', '04', '04', '02', '00', '00', '04', '02', '02', '04', '04', '02', '00', '06', '00', '06', '00', '00', '04', '02', '02', '04', '02', '04', '04', '02', '00', '00', '06', '04', '06', '00', '06', '00', '04', '00', '06', '06', '00', '04', '00', '06', '00', '06', '04', '02', '04', '02', '02', '04', '02', '04', '04', '02', '00', '06', '02', '04', '06', '00', '06', '00', '00', '06', '06', '00', '02', '04', '00', '06', '00', '06', '02', '04', '04', '02', '04', '02', '02', '04', '06', '00', '00', '06', '00', '06', '06', '00', '00', '06', '06', '00', '06', '00', '02', '04', '02', '04', '04', '02', '06', '06', '04', '02', '02', '04', '06', '00', '02', '04', '02', '00', '06', '00', '06', '02', '06', '00', '00', '06', '06', '00', '06', '00', '00', '06', '02', '02', '00', '06', '06', '00', '02', '04', '02', '04', '06', '04', '02', '04', '02', '06', '02', '04', '04', '02', '00', '06', '00', '06', '06', '00', '06', '00', '00', '06', '00', '06', '06', '00', '02', '04', '04', '02', '04', '02', '02', '04', '06', '00', '04', '02', '06', '00', '00', '06', '00', '06', '06', '00', '06', '00', '00', '06', '04', '02', '04', '02', '02', '04', '02', '04', '04', '02', '02', '04', '06', '00', '06', '00', '04', '00', '06', '04', '00', '06', '06', '00', '06', '00', '00', '06', '04', '04', '02', '00', '02', '04', '02', '04', '00', '04', '02', '00', '06', '00', '02', '04', '00', '06', '02', '04', '06', '00', '06', '00', '04', '02', '00', '02', '04', '02', '04', '00', '04', '02', '00', '06', '06', '00', '04', '04', '06', '00', '00', '06', '04', '02', '00', '06', '00', '06', '02', '04', '00', '06', '04', '02', '02', '04', '02', '04', '04', '02', '00', '06', '06', '00', '06', '00', '00', '06', '00', '06', '04', '02', '00', '06', '00', '06', '02', '04', '04', '02', '04', '02', '02', '04', '06', '00', '00', '06', '00', '06', '06', '00', '02', '04', '06', '00', '06', '00', '06', '04', '02', '06', '00', '02', '04', '06', '04', '02', '04', '02', '06', '02', '04', '02', '06', '00', '00', '06', '00', '06', '06', '00', '02', '02', '06', '00', '00', '06', '02', '06', '00', '02', '00', '06', '00', '06', '02', '06', '00', '06', '02', '04', '04', '02', '04', '02', '02', '04', '06', '02', '04', '06', '00', '06', '00', '06', '00', '06', '06', '00', '04', '02', '00', '06', '00', '06', '06', '00', '02', '04', '04', '02', '04', '02', '02', '04', '06', '00', '06', '00', '02', '04', '06', '00', '00', '06', '06', '00', '06', '00', '00', '06', '04', '02', '02', '04', '02', '04', '04', '02', '04', '02', '06', '00', '06', '00', '06', '00', '00', '06', '04', '02', '04', '06', '00', '06', '00', '04', '00', '06', '04', '02', '02', '04', '00', '00', '02', '04', '04', '02', '00', '06', '00', '06', '04', '02', '04', '02', '02', '04', '00', '00', '02', '04', '04', '02', '00', '06', '00', '06', '04', '02', '06', '00', '00', '06', '04', '02', '04', '06', '00', '06', '00', '04', '00', '06', '04', '02', '02', '04', '02', '04', '04', '02', '04', '02', '06', '00', '06', '00', '06', '00', '00', '06', '06', '00', '06', '00', '00', '06', '02', '04', '04', '02', '04', '02', '02', '04', '06', '00', '06', '00', '02', '04', '00', '06', '06', '00', '04', '02', '00', '06', '00', '06', '06', '00', '06', '02', '04', '04', '02', '04', '02', '02', '04', '06', '02', '04', '06', '00', '06', '00', '06', '00', '06', '02', '06', '00', '02', '00', '06', '00', '06', '02', '06', '00', '02', '06', '00', '00', '06', '00', '06', '06', '00', '02', '02', '06', '00', '06', '00', '06', '04', '02', '06', '00', '02', '04', '06', '04', '02', '04', '02', '06', '02', '04', '06', '00', '00', '06', '00', '06', '06', '00', '02', '04', '06', '00', '04', '02', '00', '06', '00', '06', '02', '04', '04', '02', '04', '02', '02', '04', '00', '06', '06', '00', '06', '00', '00', '06', '00', '06', '00', '06', '02', '04', '00', '06', '04', '02', '02', '04', '02', '04', '04', '02', '00', '06', '06', '00', '04', '04', '06', '00', '00', '06', '04', '02', '00', '06', '02', '04', '06', '00', '06', '00', '04', '02', '00', '02', '04', '02', '04', '00', '04', '02', '04', '02', '00', '02', '04', '02', '04', '00', '04', '02', '00', '06', '00', '02', '04', '00', '06', '04', '00', '06', '04', '00', '06', '06', '00', '06', '00', '00', '06', '04', '04', '02', '02', '04', '02', '04', '04', '02', '02', '04', '06', '00', '06', '00', '00', '06', '00', '06', '06', '00', '06', '00', '00', '06', '04', '02', '02', '04', '04', '02', '04', '02', '02', '04', '06', '00', '04', '02', '06', '00', '06', '00', '06', '00', '00', '06', '00', '06', '06', '00', '02', '04', '06', '04', '02', '04', '02', '06', '02', '04', '04', '02', '00', '06', '00', '06', '06', '00', '06', '00', '00', '06', '02', '02', '00', '06', '06', '00', '02', '04', '06', '00', '02', '04', '02', '00', '06', '00', '06', '02', '06', '00', '00', '06', '06', '00', '06', '00', '02', '04', '02', '04', '04', '02', '06', '06', '04', '02', '02', '04', '06', '00', '00', '06', '00', '06', '06', '00', '00', '06', '02', '04', '00', '06', '00', '06', '02', '04', '04', '02', '04', '02', '02', '04', '00', '06', '02', '04', '06', '00', '06', '00', '00', '06', '06', '00', '00', '06', '00', '06', '04', '02', '04', '02', '02', '04', '02', '04', '04', '02', '00', '06', '04', '06', '00', '06', '00', '04', '00', '06', '06', '00', '04', '04', '02', '00', '06', '00', '06', '00', '00', '04', '02', '02', '04', '02', '04', '04', '02', '00', '00', '04', '02', '04', '02', '00', '02', '04', '02', '04', '00', '04', '02', '00', '06', '06', '00', '04', '04', '06', '00', '00', '06', '04', '02', '04', '06', '00', '04', '02', '04', '02', '04', '02', '00', '06', '02', '04', '02', '04', '04', '02', '00', '06', '06', '00', '06', '00', '00', '06', '02', '04', '06', '00', '02', '04', '02', '04', '02', '04', '04', '02', '04', '02', '02', '04', '06', '00', '00', '06', '00', '06', '06', '00', '02', '04', '00', '06', '02', '04', '02', '04', '02', '04', '06', '00', '06', '04', '02', '04', '02', '06', '02', '04', '02', '06', '00', '00', '06', '00', '06', '06', '00', '02', '04', '02', '00', '06', '04', '02', '02', '04', '06', '00', '02', '04', '06', '00', '02', '00', '06', '00', '06', '02', '06', '00', '06', '02', '04', '04', '02', '00', '06', '04', '02', '02', '04', '06', '04', '02', '04', '02', '04', '02', '06', '00', '06', '00', '00', '06', '00', '06', '06', '00', '02', '04', '04', '02', '04', '02', '02', '04', '04', '02', '04', '02', '00', '06', '04', '02', '00', '06', '06', '00', '06', '00', '00', '06', '04', '02', '02', '04', '06', '00', '02', '04', '04', '02', '02', '04', '02', '04', '02', '04', '00', '06', '04', '00', '06', '04', '06', '00', '06', '00', '04', '00', '06', '04', '02', '02', '04', '00', '00', '02', '04', '04', '02', '02', '04', '00', '02', '04', '00', '04', '02', '02', '04', '06', '00', '02', '04', '04', '02', '00', '00', '06', '04', '06', '00', '06', '00', '04', '00', '06', '04', '02', '00', '06', '04', '02', '02', '04', '04', '02', '02', '04', '02', '04', '04', '02', '00', '06', '06', '00', '06', '00', '00', '06', '02', '04', '02', '04', '00', '06', '04', '02', '02', '04', '04', '02', '00', '06', '04', '02', '02', '04', '06', '00', '00', '06', '00', '06', '06', '00', '02', '04', '06', '00', '02', '04', '04', '02', '06', '02', '04', '04', '02', '06', '06', '04', '02', '02', '04', '06', '00', '02', '00', '06', '00', '06', '02', '06', '00', '04', '02', '04', '02', '06', '00', '02', '02', '04', '02', '00', '06', '02', '04', '06', '00', '00', '06', '02', '02', '00', '06', '06', '00', '02', '04', '06', '04', '02', '04', '02', '06', '02', '04', '06', '02', '04', '04', '02', '00', '06', '04', '02', '06', '00', '00', '06', '00', '06', '06', '00', '02', '04', '06', '00', '04', '02', '04', '02', '02', '04', '02', '04', '04', '02', '06', '00', '04', '02', '00', '06', '06', '00', '06', '00', '00', '06', '04', '02', '02', '04', '02', '04', '04', '02', '04', '02', '02', '04', '06', '00', '02', '04', '04', '00', '06', '06', '00', '06', '00', '00', '06', '04', '04', '02', '00', '06', '00', '02', '04', '02', '04', '00', '04', '02', '04', '02', '04', '02', '00', '06', '00', '02', '04', '02', '04', '00', '04', '02', '04', '02', '02', '04', '06', '00', '02', '04', '04', '00', '06', '06', '00', '06', '00', '00', '06', '04', '04', '02', '02', '04', '02', '04', '04', '02', '04', '02', '04', '02', '06', '00', '04', '02', '00', '06', '06', '00', '06', '00', '00', '06', '02', '04', '06', '00', '04', '02', '04', '02', '02', '04', '02', '04', '04', '02', '00', '06', '04', '02', '06', '00', '00', '06', '00', '06', '06', '00', '02', '04', '06', '04', '02', '04', '02', '06', '02', '04', '06', '02', '04', '02', '04', '02', '00', '06', '02', '04', '06', '00', '00', '06', '02', '02', '00', '06', '06', '00', '06', '00', '02', '00', '06', '00', '06', '02', '06', '00', '04', '02', '04', '02', '06', '00', '02', '04', '02', '06', '02', '04', '04', '02', '06', '06', '04', '02', '02', '04', '06', '00', '00', '06', '00', '06', '06', '00', '02', '04', '06', '00', '02', '04', '04', '02', '02', '04', '04', '02', '00', '06', '04', '02', '02', '04', '00', '06', '06', '00', '06', '00', '00', '06', '02', '04', '02', '04', '00', '06', '02', '04', '04', '02', '02', '04', '02', '04', '04', '02', '00', '06', '04', '06', '00', '06', '00', '04', '00', '06', '04', '02', '00', '06', '04', '02', '02', '04', '00', '04', '02', '02', '04', '06', '00', '02', '04', '04', '02', '00', '04', '02', '02', '04', '00', '00', '02', '04', '04', '02', '02', '04', '00', '02', '04', '02', '04', '00', '06', '04', '00', '06', '04', '06', '00', '06', '00', '04', '00', '06', '04', '02', '02', '04', '06', '00', '02', '04', '04', '02', '02', '04', '04', '02', '00', '06', '04', '02', '00', '06', '06', '00', '06', '00', '00', '06', '02', '04', '04', '02', '04', '02', '02', '04', '04', '02', '04', '02', '04', '02', '06', '00', '06', '00', '00', '06', '00', '06', '06', '00', '06', '02', '04', '04', '02', '00', '06', '04', '02', '02', '04', '06', '04', '02', '02', '04', '06', '00', '02', '04', '06', '00', '02', '00', '06', '00', '06', '02', '06', '00', '02', '06', '00', '00', '06', '00', '06', '06', '00', '02', '04', '02', '00', '06', '04', '02', '02', '04', '02', '04', '06', '00', '06', '04', '02', '04', '02', '06', '02', '04', '06', '00', '00', '06', '00', '06', '06', '00', '02', '04', '00', '06', '02', '04', '02', '04', '02', '04', '04', '02', '04', '02', '02', '04', '00', '06', '06', '00', '06', '00', '00', '06', '02', '04', '06', '00', '02', '04', '04', '02', '04', '02', '00', '06', '02', '04', '02', '04', '04', '02', '00', '06', '06', '00', '04', '04', '06', '00', '00', '06', '04', '02', '04', '06', '00', '04', '02', '00', '04', '02', '04', '02', '00', '02', '04', '02', '04', '00', '04', '02', '02', '04', '02', '04', '04', '00', '04', '02', '04', '02', '00', '02', '04', '04', '06', '00', '00', '06', '02', '02', '00', '06', '06', '00', '04', '02', '04', '00', '00', '06', '00', '02', '04', '02', '04', '02', '04', '04', '02', '04', '02', '06', '00', '02', '04', '06', '00', '00', '06', '00', '06', '06', '00', '04', '02', '00', '06', '04', '02', '04', '02', '04', '02', '02', '04', '02', '04', '04', '02', '00', '06', '06', '00', '06', '00', '00', '06', '04', '02', '06', '00', '04', '02', '06', '04', '02', '04', '02', '06', '02', '04', '02', '04', '06', '00', '06', '04', '02', '04', '00', '06', '06', '00', '02', '02', '06', '00', '00', '06', '04', '06', '02', '02', '04', '06', '00', '02', '04', '06', '04', '02', '06', '06', '00', '06', '04', '02', '02', '00', '06', '02', '04', '06', '00', '06', '00', '04', '02', '00', '06', '04', '02', '02', '04', '06', '00', '06', '06', '02', '04', '04', '02', '02', '04', '06', '00', '06', '02', '04', '02', '04', '00', '06', '06', '00', '06', '00', '00', '06', '04', '02', '02', '04', '02', '04', '04', '02', '02', '04', '02', '04', '06', '00', '02', '04', '06', '00', '00', '06', '00', '06', '06', '00', '02', '04', '04', '02', '00', '06', '04', '02', '02', '04', '04', '02', '00', '06', '00', '04', '02', '04', '02', '00', '06', '00', '02', '04', '00', '06', '00', '06', '04', '02', '06', '00', '00', '02', '04', '04', '02', '04', '04', '02', '02', '04', '00', '04', '02', '04', '02', '00', '02', '04', '04', '02', '00', '00', '06', '00', '04', '02', '02', '04', '06', '00', '04', '02', '00', '06', '00', '06', '02', '04', '06', '00', '02', '04', '00', '00', '06', '00', '04', '02', '04', '04', '02', '02', '04', '04', '02', '04', '02', '02', '04', '06', '00', '00', '06', '00', '06', '06', '00', '06', '00', '04', '02', '04', '02', '02', '04', '04', '02', '02', '04', '06', '00', '02', '04', '04', '02', '00', '06', '06', '00', '06', '00', '00', '06', '04', '02', '00', '06', '04', '02', '00', '02', '04', '06', '04', '02', '02', '04', '02', '04', '02', '04', '02', '06', '00', '06', '04', '02', '06', '00', '06', '00', '02', '04', '00', '06', '04', '00', '06', '06', '02', '04', '02', '04', '06', '04', '02', '06', '06', '06', '00', '04', '04', '02', '02', '00', '06', '06', '00', '04', '04', '06', '00', '00', '06', '02', '02', '04', '02', '06', '02', '04', '02', '04', '06', '04', '02', '04', '02', '00', '02', '04', '06', '00', '02', '04', '00', '06', '06', '00', '06', '00', '00', '06', '04', '02', '02', '04', '02', '04', '00', '06', '04', '02', '04', '02', '02', '04', '00', '06', '02', '04', '06', '00', '00', '06', '00', '06', '06', '00', '02', '04', '04', '02', '04', '02', '02', '04', '02', '04', '00', '04', '02', '04', '00', '06', '04', '02', '00', '02', '06', '00', '00', '06', '04', '04', '00', '06', '06', '00', '02', '02', '04', '00', '04', '02', '04', '02', '06', '00', '00', '02', '04', '00', '02', '04', '02', '04', '00', '04', '02', '04', '02', '00', '06', '00', '02', '04', '00', '02', '04', '00', '04', '04', '02', '00', '06', '04', '02', '00', '02', '06', '00', '00', '06', '04', '04', '00', '06', '06', '00', '02', '04', '04', '02', '04', '02', '02', '04', '02', '04', '02', '04', '00', '06', '02', '04', '06', '00', '00', '06', '00', '06', '06', '00', '04', '02', '02', '04', '02', '04', '00', '06', '04', '02', '04', '02', '02', '04', '06', '00', '02', '04', '00', '06', '06', '00', '06', '00', '00', '06', '04', '02', '02', '06', '02', '04', '02', '04', '06', '04', '02', '04', '02', '04', '02', '06', '06', '00', '06', '04', '02', '02', '00', '06', '06', '00', '04', '04', '06', '00', '00', '06', '02', '00', '06', '04', '02', '06', '00', '06', '00', '02', '04', '00', '06', '06', '00', '06', '02', '04', '02', '04', '06', '02', '04', '06', '04', '02', '02', '04', '02', '02', '04', '04', '02', '06', '00', '06', '06', '00', '06', '00', '00', '06', '04', '02', '00', '06', '04', '02', '02', '04', '04', '02', '02', '04', '06', '00', '02', '04', '04', '02', '06', '00', '00', '06', '00', '06', '06', '00', '06', '00', '04', '02', '04', '02', '04', '02', '02', '04', '04', '02', '04', '02', '02', '04', '06', '00', '04', '00', '06', '00', '06', '02', '04', '06', '00', '02', '04', '00', '00', '06', '00', '02', '04', '04', '04', '02', '00', '02', '04', '04', '02', '00', '06', '00', '00', '04', '02', '02', '04', '00', '02', '04', '04', '02', '04', '02', '04', '02', '04', '00', '06', '04', '02', '06', '00', '00', '04', '02', '04', '02', '00', '06', '00', '02', '04', '00', '06', '00', '06', '04', '02', '06', '00', '02', '04', '04', '02', '00', '06', '04', '02', '02', '04', '04', '02', '02', '04', '06', '00', '02', '04', '06', '00', '00', '06', '00', '06', '06', '00', '04', '02', '02', '04', '02', '04', '04', '02', '02', '04', '00', '06', '02', '04', '02', '04', '00', '06', '06', '00', '06', '00', '00', '06', '04', '02', '02', '04', '06', '06', '00', '06', '02', '04', '04', '02', '02', '04', '06', '04', '02', '06', '06', '00', '06', '02', '04', '02', '00', '06', '02', '04', '06', '00', '06', '00', '04', '02', '00', '06', '04', '00', '06', '06', '00', '02', '02', '06', '00', '00', '06', '04', '06', '02', '04', '02', '06', '00', '02', '04', '06', '06', '04', '02', '04', '02', '06', '02', '04', '02', '04', '00', '06', '06', '04', '02', '00', '06', '06', '00', '06', '00', '00', '06', '04', '02', '06', '00', '04', '02', '04', '02', '04', '02', '02', '04', '02', '04', '04', '02', '06', '00', '00', '06', '00', '06', '06', '00', '04', '02', '00', '06', '04', '02', '02', '04', '02', '04', '04', '02', '04', '02', '06', '00', '02', '04', '04', '06', '00', '00', '06', '02', '02', '00', '06', '06', '00', '04', '02', '04', '00', '00', '00', '06', '02', '04', '02', '04', '06', '04', '02', '04', '00', '04', '02', '04', '02', '00', '02', '04']\n",
      "['06', '00', '00', '06', '02', '06', '02', '06', '02', '06', '06', '04', '02', '06', '02', '04', '00', '06', '06', '00', '02', '00', '02', '04', '02', '04', '06', '04', '02', '00', '06', '06', '00', '00', '04', '04', '04', '02', '04', '04', '02', '06', '00', '00', '00', '00', '00', '06', '04', '04', '06', '00', '02', '04', '00', '04', '02', '00', '02', '02', '04', '00', '02', '04', '02', '00', '00', '04', '06', '04', '00', '06', '00', '00', '00', '06', '00', '04', '02', '02', '04', '04', '04', '00', '04', '06', '00', '00', '06', '04', '02', '02', '06', '04', '04', '02', '00', '02', '06', '00', '00', '06', '02', '06', '04', '04', '02', '06', '06', '02', '06', '02', '06', '00', '02', '00', '06', '00', '06', '06', '00', '02', '00', '02', '02', '04', '06', '06', '04', '02', '02', '04', '00', '06', '06', '00', '02', '06', '02', '02', '02', '04', '04', '02', '00', '06', '06', '00', '04', '06', '04', '02', '02', '04', '04', '00', '02', '00', '06', '04', '00', '00', '06', '00', '00', '04', '04', '00', '00', '00', '00', '02', '02', '04', '02', '04', '00', '02', '00', '00', '00', '04', '04', '00', '00', '00', '06', '06', '00', '00', '04', '04', '02', '00', '02', '04', '02', '04', '06', '04', '06', '00', '00', '06', '04', '02', '02', '04', '02', '06', '02', '02', '06', '00', '00', '06', '02', '04', '04', '02', '06', '04', '02', '06', '02', '02', '00', '00', '06', '06', '00', '02', '02', '00', '00', '06', '06', '00', '02', '04', '04', '02', '06', '04', '02', '06', '06', '02', '02', '06', '00', '00', '06', '04', '02', '02', '04', '02', '04', '06', '04', '06', '00', '00', '06', '04', '02', '00', '02', '04', '02', '04', '00', '04', '00', '00', '06', '06', '00', '04', '00', '02', '00', '04', '02', '00', '00', '00', '00', '04', '00', '02', '02', '00', '06', '00', '00', '04', '00', '06', '00', '04', '00', '04', '02', '02', '04', '04', '00', '02', '00', '06', '06', '00', '04', '06', '04', '02', '02', '04', '04', '02', '00', '06', '06', '00', '02', '06', '02', '02', '04', '06', '06', '04', '02', '02', '04', '00', '06', '06', '00', '02', '00', '02', '02', '02', '06', '06', '00', '02', '00', '06', '06', '02', '04', '04', '02', '06', '06', '02', '00', '02', '06', '00', '00', '06', '04', '02', '02', '06', '04', '04', '04', '00', '04', '06', '00', '00', '06', '04', '02', '02', '04', '04', '04', '06', '04', '00', '00', '06', '00', '00', '06', '00', '02', '04', '00', '02', '04', '02', '00', '00', '00', '02', '04', '00', '02', '00', '02', '04', '06', '00', '00', '00', '00', '06', '00', '04', '04', '06', '04', '02', '04', '04', '02', '00', '06', '06', '00', '00', '04', '04', '04', '02', '04', '06', '04', '02', '00', '06', '06', '00', '02', '00', '02', '06', '06', '04', '02', '02', '06', '04', '06', '00', '00', '06', '02', '06', '02', '02', '00', '06', '06', '00', '02', '04', '04', '02', '00', '06', '04', '06', '02', '04', '04', '02', '06', '00', '00', '06', '04', '06', '04', '02', '04', '02', '00', '02', '04', '06', '00', '00', '06', '02', '02', '06', '02', '04', '02', '02', '04', '00', '06', '06', '04', '00', '02', '04', '00', '04', '02', '00', '04', '00', '02', '04', '06', '00', '04', '02', '04', '00', '06', '02', '02', '04', '00', '04', '00', '02', '04', '00', '04', '02', '06', '00', '04', '00', '06', '02', '04', '04', '02', '02', '02', '06', '02', '00', '06', '06', '00', '02', '00', '04', '04', '02', '02', '04', '06', '04', '00', '06', '06', '00', '04', '02', '02', '04', '04', '06', '06', '02', '04', '00', '04', '02', '06', '00', '00', '06', '00', '02', '06', '06', '00', '04', '02', '02', '06', '04', '06', '06', '02', '06', '02', '00', '04', '04', '02', '06', '00', '00', '06', '04', '00', '04', '04', '04', '02', '02', '04', '06', '00', '00', '06', '02', '00', '02', '04', '04', '06', '02', '02', '04', '00', '06', '06', '00', '04', '02', '06', '02', '04', '02', '00', '00', '02', '04', '04', '02', '04', '02', '02', '04', '00', '02', '00', '04', '02', '02', '04', '06', '06', '00', '00', '06', '02', '04', '06', '04', '02', '04', '02', '00', '02', '00', '06', '06', '00', '02', '04', '04', '02', '04', '04', '04', '00', '00', '06', '06', '00', '04', '02', '00', '02', '06', '04', '02', '06', '06', '04', '06', '02', '02', '04', '06', '00', '00', '06', '02', '04', '02', '06', '02', '04', '06', '00', '00', '02', '06', '04', '02', '00', '06', '02', '04', '06', '02', '06', '04', '04', '00', '00', '06', '06', '00', '02', '04', '04', '02', '04', '02', '00', '02', '00', '06', '06', '00', '02', '04', '06', '04', '02', '04', '04', '02', '02', '04', '06', '06', '00', '00', '06', '04', '02', '02', '04', '00', '02', '00', '02', '00', '00', '02', '04', '04', '02', '00', '06', '06', '00', '04', '02', '06', '02', '04', '04', '04', '06', '02', '02', '04', '06', '00', '00', '06', '02', '00', '02', '04', '04', '02', '02', '04', '06', '00', '00', '06', '04', '00', '04', '06', '02', '06', '02', '06', '00', '04', '04', '02', '00', '06', '02', '06', '00', '04', '02', '06', '02', '04', '02', '04', '00', '04', '02', '06', '00', '00', '06', '04', '02', '02', '04', '04', '06', '06', '04', '06', '04', '00', '06', '06', '00', '02', '00', '04', '04', '02', '02', '02', '06', '02', '00', '06', '06', '00', '02', '04', '04', '02', '02', '02', '04', '04', '00', '02', '06', '04', '00', '00', '06', '00', '04', '06', '02', '02', '04', '00', '00', '04', '04', '00', '00', '02', '04', '06', '04', '00', '02', '00', '06', '06', '00', '04', '02', '00', '04', '04', '02', '02', '04', '02', '02', '04', '06', '00', '00', '06', '02', '02', '06', '02', '04', '02', '00', '02', '04', '06', '00', '00', '06', '04', '06', '04', '06', '04', '06', '02', '04', '04', '02', '00', '06', '06', '00', '02', '04', '04', '02', '00', '00', '06', '06', '00', '02', '02', '00', '00', '06', '04', '06', '00', '02', '04', '04', '02', '06', '00', '00', '06', '04', '06', '02', '00', '00', '04', '02', '02', '04', '06', '00', '00', '06', '06', '06', '02', '06', '04', '02', '02', '04', '00', '06', '06', '04', '00', '02', '04', '00', '04', '00', '04', '06', '06', '00', '02', '04', '00', '04', '02', '04', '00', '02', '02', '04', '00', '06', '06', '04', '00', '04', '00', '04', '06', '02', '00', '04', '00', '06', '02', '04', '04', '02', '02', '06', '06', '06', '00', '06', '06', '00', '02', '04', '04', '02', '00', '00', '02', '06', '00', '04', '06', '06', '00', '04', '02', '02', '04', '04', '00', '06', '06', '00', '02', '00', '02', '06', '00', '00', '06', '00', '04', '02', '06', '06', '00', '02', '02', '06', '06', '00', '00', '06', '02', '06', '02', '04', '04', '02', '06', '00', '00', '06', '00', '04', '00', '00', '04', '02', '02', '04', '06', '02', '00', '00', '06', '00', '06', '06', '04', '04', '02', '02', '04', '00', '06', '06', '00', '04', '06', '04', '02', '06', '00', '00', '06', '02', '04', '04', '02', '04', '02', '02', '04', '06', '00', '02', '00', '06', '04', '04', '06', '06', '00', '00', '06', '02', '04', '04', '02', '04', '06', '06', '00', '00', '06', '06', '00', '02', '02', '04', '04', '02', '00', '04', '00', '00', '00', '06', '06', '00', '04', '02', '02', '06', '04', '02', '06', '00', '00', '06', '06', '02', '02', '06', '00', '00', '06', '02', '04', '02', '06', '02', '06', '00', '00', '02', '06', '04', '04', '02', '06', '02', '04', '06', '02', '00', '00', '06', '00', '00', '06', '06', '00', '02', '04', '04', '02', '00', '04', '00', '00', '00', '06', '06', '00', '02', '02', '04', '04', '02', '04', '06', '06', '04', '04', '06', '06', '00', '00', '06', '04', '02', '02', '04', '06', '00', '02', '00', '06', '02', '06', '00', '00', '06', '02', '04', '04', '02', '00', '06', '06', '00', '04', '06', '04', '06', '06', '04', '04', '02', '02', '04', '06', '02', '00', '00', '06', '00', '04', '00', '00', '04', '02', '02', '04', '06', '00', '00', '06', '00', '06', '00', '00', '02', '06', '02', '06', '04', '04', '02', '00', '04', '06', '02', '06', '00', '02', '06', '02', '02', '00', '02', '06', '00', '00', '06', '04', '02', '02', '04', '04', '00', '06', '06', '00', '06', '00', '04', '06', '06', '00', '02', '04', '04', '02', '00', '00', '02', '06', '00', '06', '06', '00', '02', '04', '04', '02', '02', '06', '06', '04', '04', '00', '06', '02', '04', '00', '00', '06', '00', '04', '02', '02', '04', '00', '06', '06', '00', '04', '04', '00', '06', '06', '00', '02', '04', '04', '00', '02', '00', '06', '06', '00', '04', '02', '00', '04', '04', '06', '02', '06', '04', '02', '02', '04', '06', '00', '00', '06', '06', '02', '00', '00', '04', '02', '02', '04', '06', '00', '00', '06', '04', '06', '00', '06', '04', '06', '00', '02', '04', '04', '02', '00', '06', '06', '00', '02', '02', '00', '06', '00', '00', '02', '06', '02', '02', '06', '02', '06', '04', '02', '06', '02', '04', '02', '00', '06', '06', '00', '00', '06', '06', '04', '02', '04', '04', '02', '00', '06', '06', '00', '00', '00', '04', '00', '02', '04', '04', '02', '06', '00', '00', '06', '04', '04', '00', '06', '00', '00', '00', '02', '00', '04', '02', '02', '04', '04', '02', '02', '04', '02', '00', '00', '00', '00', '04', '06', '00', '04', '00', '06', '06', '00', '04', '02', '02', '04', '00', '00', '04', '00', '06', '00', '00', '06', '04', '02', '02', '04', '04', '06', '06', '00', '06', '00', '00', '02', '06', '06', '04', '02', '02', '04', '06', '02', '06', '02', '02', '00', '06', '02', '06', '00', '06', '00', '00', '06', '02', '00', '06', '02', '04', '06', '06', '02', '04', '04', '02', '00', '06', '06', '00', '06', '06', '06', '02', '02', '04', '04', '02', '00', '06', '06', '04', '00', '06', '02', '00', '00', '02', '04', '04', '02', '06', '04', '00', '00', '06', '04', '04', '00', '00', '00', '00', '00', '00', '02', '04', '04', '02', '02', '04', '00', '04', '02', '00', '00', '00', '00', '00', '04', '04', '00', '06', '06', '00', '04', '04', '02', '02', '04', '00', '00', '02', '06', '04', '06', '00', '00', '06', '04', '02', '02', '04', '06', '02', '06', '06', '06', '00', '00', '06', '02', '04', '04', '02', '06', '04', '06', '02', '02', '06', '00', '00', '06', '06', '00', '02', '02', '06', '00', '00', '06', '06', '00', '02', '04', '04', '02', '06', '04', '06', '06', '06', '00', '00', '06', '04', '02', '02', '04', '06', '02', '06', '06', '04', '06', '00', '00', '06', '04', '02', '02', '04', '00', '00', '02', '04', '00', '04', '00', '06', '06', '04', '00', '02', '04', '00', '04', '02', '00', '00', '00', '00', '00', '00', '00', '00', '00', '02', '04', '04', '02', '06', '00', '04', '00', '06', '04', '00', '04', '02', '00', '00', '02', '04', '04', '02', '00', '06', '06', '04', '00', '06', '06', '06', '02', '02', '04', '04', '02', '00', '06', '06', '00', '06', '04', '06', '06', '02', '04', '04', '02', '06', '00', '00', '06', '02', '00', '06', '02', '02', '02', '00', '02', '06', '06', '00', '06', '04', '02', '02', '04', '06', '06', '02', '00', '06', '00', '00', '02', '06', '04', '02', '02', '04', '04', '06', '06', '00', '06', '00', '00', '06', '04', '02', '02', '04', '00', '00', '04', '04', '06', '00', '04', '00', '06', '06', '00', '04', '02', '02', '04', '02', '00', '00', '00', '00', '00', '00', '00', '02', '00', '04', '02', '02', '04', '06', '00', '00', '06', '04', '04', '00', '06', '00', '04', '00', '02', '04', '04', '02', '00', '06', '06', '00', '00', '06', '06', '04', '02', '04', '04', '02', '02', '00', '06', '06', '00', '00', '06', '06', '04', '02', '06', '02', '04', '06', '00', '00', '06', '02', '02', '02']\n"
     ]
    }
   ],
   "source": [
    "print(type(train_ids), np.shape(train_ids))\n",
    "print(type(test_ids), np.shape(test_ids))\n",
    "print(train_ids)\n",
    "print(test_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e201b0",
   "metadata": {},
   "source": [
    "Precompute Features ONCE (cache), then 100 Bootstrap runs total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a75b7baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precompute train features(once)...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3675/3675 [00:24<00:00, 152.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precompute test features(once)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:12<00:00, 155.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# feature cache (compute once per file)\n",
    "\n",
    "print(\"\\nPrecompute train features(once)...\\n\")\n",
    "train_feat_all = {f: compute_logmel_context(f) for f in tqdm(train_files)}\n",
    "\n",
    "print(\"\\nPrecompute test features(once)...\")\n",
    "test_feat_all = {f: compute_logmel_context(f) for f in tqdm(test_files)}\n",
    "\n",
    "# drop NULL files globally\n",
    "valid_train = [f for f, feat in train_feat_all.items() if feat is not None]\n",
    "valid_test = [f for f, feat in test_feat_all.items() if feat is not None]\n",
    "\n",
    "# filter file lists\n",
    "train_files_v = valid_train\n",
    "test_files_v = valid_test\n",
    "\n",
    "# filter IDs using the same validity rule \n",
    "train_file_to_id = dict(zip(train_files, train_ids))\n",
    "test_file_to_id = dict(zip(test_files, test_ids))\n",
    "\n",
    "train_ids_v = [train_file_to_id[f] for f in train_files_v]\n",
    "test_ids_v = [test_file_to_id[f] for f in test_files_v]\n",
    "\n",
    "# filter feature caches to valid entries only\n",
    "train_feat_all = {f: train_feat_all[f] for f in train_files_v}\n",
    "test_feat_all = {f: test_feat_all[f] for f in test_files_v}\n",
    "\n",
    "# unique sorted training IDs\n",
    "unique_ids = sorted(set(train_ids_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c7779d",
   "metadata": {},
   "source": [
    "Bootstrap settings (100 runs total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02fb58e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 100\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ce13e6",
   "metadata": {},
   "source": [
    "Helper: training+evaluation for one ID given bootstrapped train file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a25a9c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_id_if(id, \n",
    "                  boot_train_files_for_id, \n",
    "                  max_frames_per_clip=None,\n",
    "                  epochs=20, \n",
    "                  batch_size=256, \n",
    "                  verbose=0):\n",
    "    \"\"\"\n",
    "    Bootstrap run for one ID using an autoencoder for anomaly detection.\n",
    "    \"\"\"\n",
    "    train_feat_list = [train_feat_all[f] for f in boot_train_files_for_id if f in train_feat_all] \n",
    "    if len(train_feat_all)==0:\n",
    "        return None\n",
    "    \n",
    "    # ---------------------------------------------------\n",
    "    # Optional speed-up: cap frames per train clip\n",
    "    # ---------------------------------------------------\n",
    "    if max_frames_per_clip is not None:\n",
    "        max_frames_per_clip = int(max_frames_per_clip)\n",
    "        new_train_feat_list = []\n",
    "\n",
    "        for M in train_feat_list:\n",
    "            if M is None or M.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            if M.shape[0] <= max_frames_per_clip:\n",
    "                new_train_feat_list.append(M)\n",
    "            else:\n",
    "                idx = np.random.choice(\n",
    "                    M.shape[0], max_frames_per_clip, replace=False\n",
    "                )\n",
    "                new_train_feat_list.append(M[idx, :])\n",
    "\n",
    "        train_feat_list = new_train_feat_list\n",
    "\n",
    "        if len(train_feat_list) == 0:\n",
    "            return None\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Stack training frames\n",
    "    # ---------------------------------------------------\n",
    "    X_train = np.vstack(train_feat_list)\n",
    "\n",
    "    if X_train.shape[0] < 2:\n",
    "        return None\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Fixed test set for this ID\n",
    "    # ---------------------------------------------------\n",
    "    test_idx = [i for i, tid in enumerate(test_ids_v) if tid == id]\n",
    "\n",
    "    if len(test_idx) == 0:\n",
    "        return None\n",
    "\n",
    "    test_files_id = [test_files_v[i] for i in test_idx]\n",
    "\n",
    "    test_feat_list = [\n",
    "        test_feat_all[f]\n",
    "        for f in test_files_id\n",
    "        if f in test_feat_all and test_feat_all[f] is not None\n",
    "    ]\n",
    "\n",
    "    if len(test_feat_list) == 0:\n",
    "        return None\n",
    "\n",
    "    X_test = np.vstack(test_feat_list)\n",
    "\n",
    "    if X_test.shape[0] < 2:\n",
    "        return None\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Clip labels from filename\n",
    "    # ---------------------------------------------------\n",
    "    y_test_clip = np.array([\n",
    "        1 if \"anomaly\" in os.path.basename(f).lower() else 0\n",
    "        for f in test_files_id\n",
    "    ])\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Frame → clip grouping\n",
    "    # ---------------------------------------------------\n",
    "    frame_counts = [M.shape[0] for M in test_feat_list]\n",
    "    test_groups = np.repeat(\n",
    "        np.arange(len(frame_counts)), frame_counts\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Train autoencoder on NORMAL frames only\n",
    "    # ---------------------------------------------------\n",
    "    model = build_model(input_dim)\n",
    "\n",
    "    model.fit(\n",
    "        X_train, X_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Reconstruction error = anomaly score\n",
    "    # ---------------------------------------------------\n",
    "    X_test_hat = model.predict(X_test, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    frame_scores = np.mean(\n",
    "        np.square(X_test - X_test_hat),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Aggregate frame scores → clip scores\n",
    "    # ---------------------------------------------------\n",
    "    clip_scores = np.array([\n",
    "        frame_scores[test_groups == g].mean()\n",
    "        for g in np.unique(test_groups)\n",
    "    ])\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Metrics\n",
    "    # ---------------------------------------------------\n",
    "    auc_val = roc_auc_score(y_test_clip, clip_scores)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test_clip, clip_scores)\n",
    "    mask = fpr <= 0.1  # specificity ≥ 0.9\n",
    "    pauc_val = np.trapezoid(tpr[mask], fpr[mask]) / 0.1\n",
    "\n",
    "    return {\n",
    "        \"auc\": float(auc_val),\n",
    "        \"pauc\": float(pauc_val)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6164389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911\n"
     ]
    }
   ],
   "source": [
    "id0 = unique_ids[0]\n",
    "boot_train_files_for_id = [\n",
    "    f for f, tid in zip(train_files_v, train_ids_v)\n",
    "    if tid == id0\n",
    "]\n",
    "print(len(boot_train_files_for_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8117c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 124.2983\n",
      "Epoch 2/5\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 12.3019\n",
      "Epoch 3/5\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 11.1441\n",
      "Epoch 4/5\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - loss: 10.6463\n",
      "Epoch 5/5\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 10.3357\n",
      "{'auc': 0.5622358722358722, 'pauc': 0.04447174447174447}\n"
     ]
    }
   ],
   "source": [
    "result = run_one_id_if(\n",
    "    id=id0,\n",
    "    boot_train_files_for_id=boot_train_files_for_id,\n",
    "    max_frames_per_clip=500,   # optional\n",
    "    epochs=5,                  # keep small for testing\n",
    "    batch_size=256,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2271e3d8",
   "metadata": {},
   "source": [
    "Main bootstrap loop (100 runs total) + save all AUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d2e4fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bootstrap runs: 100%|██████████| 400/400 [29:14:38<00:00, 263.20s/it]    \n"
     ]
    }
   ],
   "source": [
    "# results_df <- data.frame(...)\n",
    "results_df = []\n",
    "\n",
    "# total number of iterations for progress bar\n",
    "total_iters = R * len(unique_ids)\n",
    "\n",
    "k = 0\n",
    "\n",
    "with tqdm(total=total_iters, desc=\"Bootstrap runs\") as pb:\n",
    "    for r in range(1, R + 1):   # R runs, 1-based like R\n",
    "        for id in unique_ids:\n",
    "\n",
    "            # ---------------------------------------------------\n",
    "            # Select training indices for this ID\n",
    "            # ---------------------------------------------------\n",
    "            train_idx = [i for i, tid in enumerate(train_ids_v) if tid == id]\n",
    "\n",
    "            if len(train_idx) == 0:\n",
    "                k += 1\n",
    "                pb.update(1)\n",
    "                continue\n",
    "\n",
    "            train_files_id = [train_files_v[i] for i in train_idx]\n",
    "\n",
    "            # ---------------------------------------------------\n",
    "            # Bootstrap resampling of training CLIPS\n",
    "            # ---------------------------------------------------\n",
    "            boot_train_files_for_id = np.random.choice(\n",
    "                train_files_id,\n",
    "                size=len(train_files_id),\n",
    "                replace=True\n",
    "            ).tolist()\n",
    "\n",
    "            # ---------------------------------------------------\n",
    "            # Run one bootstrap experiment for this ID\n",
    "            # ---------------------------------------------------\n",
    "            out = run_one_id_if(\n",
    "                id=id,\n",
    "                boot_train_files_for_id=boot_train_files_for_id,\n",
    "                max_frames_per_clip=None\n",
    "            )\n",
    "\n",
    "            if out is not None:\n",
    "                results_df.append({\n",
    "                    \"run\":  r,\n",
    "                    \"id\":   id,\n",
    "                    \"auc\":  out[\"auc\"],\n",
    "                    \"pauc\": out[\"pauc\"]\n",
    "                })\n",
    "\n",
    "            k += 1\n",
    "            pb.update(1)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Convert to DataFrame\n",
    "# ---------------------------------------------------\n",
    "results_df = pd.DataFrame(results_df)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Save raw results (one row per run + id)\n",
    "# ---------------------------------------------------\n",
    "results_df.to_pickle(\"bootstrap_if_results.pkl\")\n",
    "results_df.to_csv(\"bootstrap_if_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "668531eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  auc_mean    auc_sd  pauc_mean   pauc_sd\n",
      "0  00  0.565992  0.012174   0.046715  0.007923\n",
      "1  02  0.743641  0.027012   0.178844  0.022665\n",
      "2  04  0.575886  0.018276   0.088161  0.014039\n",
      "3  06  0.838256  0.024190   0.299640  0.027758\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.read_pickle(\"bootstrap_if_results.pkl\")\n",
    "summary_by_id = (\n",
    "    results_df\n",
    "    .groupby(\"id\")\n",
    "    .agg(\n",
    "        auc_mean=(\"auc\", \"mean\"),\n",
    "        auc_sd=(\"auc\", \"std\"),\n",
    "        pauc_mean=(\"pauc\", \"mean\"),\n",
    "        pauc_sd=(\"pauc\", \"std\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(summary_by_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00d25492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean AUC across IDs per run:\n",
      "    run  mean_auc\n",
      "0     1  0.676597\n",
      "1     2  0.663341\n",
      "2     3  0.680135\n",
      "3     4  0.673567\n",
      "4     5  0.685850\n",
      "..  ...       ...\n",
      "95   96  0.676896\n",
      "96   97  0.676267\n",
      "97   98  0.673952\n",
      "98   99  0.686082\n",
      "99  100  0.684501\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "mean_auc_per_run = (\n",
    "    results_df\n",
    "    .groupby(\"run\", as_index=False)\n",
    "    .agg(mean_auc=(\"auc\", \"mean\"))\n",
    ")\n",
    "\n",
    "print(\"\\nMean AUC across IDs per run:\")\n",
    "print(mean_auc_per_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5248c5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall mean AUC: 0.680943876956309\n",
      "Overall mean pAUC: 0.1533399518984887\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOverall mean AUC:\", results_df[\"auc\"].mean())\n",
    "print(\"Overall mean pAUC:\", results_df[\"pauc\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e4835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
